# Design decisions

## Umbrella Glance Service (Split API deployment)

As mentioned in [OSSN-0090](https://wiki.openstack.org/wiki/OSSN/OSSN-0090),
when deploying Glance in a popular configuration where Glance shares a
common storage backend with Nova and/or Cinder, it is possible to open some
known attack vectors by which malicious data modification can occur. If you
choose to deploy a glance operator with Ceph as a backend then by default you
will get a split API (Internal Vs External Glance API) deployed.

- A ``user facing`` glance-api service, accessible via the Public and Admin
keystone endpoints.
- An ``internal facing only`` service, accessible via
the Internal keystone endpoint.

The user facing service is configured to not expose image locations, namely by
setting the following options in glance-api.conf:

```editorconfig
[DEFAULT]
show_image_direct_url = False
show_multiple_locations = False
```

The internal service, operating on a different port (e.g. 9293), is configured
identically to the public facing service, except for the following:

```editorconfig
[DEFAULT]
show_image_direct_url = True
show_multiple_locations = True
```

OpenStack services that use glance (cinder and nova) is configured to access
it via the new internal service. That way both cinder and nova will have
access to the image location data.


## Enable Per Tenant Quotas

Glance supports resource consumption quotas on tenants through the use of
Keystoneâ€™s unified limits functionality. In order to enable this feature, as
per the [official
documentation](https://docs.openstack.org/glance/latest/admin/quotas.html), the
`Glance CRD` exposes the `quotas` structure, where the limits defined in the
documentation can be defined. As the full [Glance example
CR](https://github.com/openstack-k8s-operators/glance-operator/blob/main/config/samples/glance_v1beta1_glance_quota.yaml)
shows, when the following is added in the top level definition, all the
`GlanceAPI` Pods will enable per tenant quotas via `use_keystone_limits =
True` in their config, that will be automatically generated by the `Glance`
operator.

```
apiVersion: glance.openstack.org/v1beta1
kind: Glance
metadata:
  name: glance
spec:
  ...
  ...
  quotas:
    imageSizeTotal: 1000
    imageStageTotal: 1000
    imageCountUpload: 100
    imageCountTotal: 100
```

As a follow up work, tha Glance operator might rely on the existing `webhooks`
to enforce the `Quota` Limits values.

## Cinder backend

When using the Cinder backend the Glance containers need to run with higher
privileges than with other backends, this is due to the usage of os-brick to
connect to volumes.

Some of the things that the operator does when using Cinder as a backend are:

- Run containers as privileged
- Mount for r/w more directories
- Pod shares the PID namespace with the host
- Run CLI commands with the binaries present in the host

OpenShift deployments are different from TripleO deployments.  For example, in
TripleO we run iscsid and multipathd daemons in containers and their client
side commands are in sync (version-wise) in the service containers.  It is also
required that there are no other daemons running in the host or other
containers.

In the OpenShift world this is different, iscsid and multipathd run on the host
and therefore their version won't be in sync with the one in the openstack
service containers, which means that they won't work correctly.

The reason why OpenShift runs iscsid and multipathd in the host is so that they
can be shared between the different things that may need it: OpenShift itself,
CSI plugins, and in this case also OpenStack services.

The way to use the host iscsi and multipath services is by exiting the pod
namespace and running the commands on the host namespace using `nsenter`. Even
if this may look nasty/weird, this is the recommended way by the storage
OpenShift team and what CSI plugins do.

We are using `nsenter` through the `templates/glance/bin/run-on-host` script.
This `run-on-host` script supports 2 ways of replacing commands: being copied
or symlinked. In our case we instruct kolla to use the copying mechanism in
`templates/glance/config/glance-api-config.json`.

To be able to use `nsenter` the pod needs to share the PID namespace with the
host.


## GlanceAPI deployment

It's possible to deploy multiple GlanceAPI to serve different workloads. When a
Glance CR is created, if not explicitly specified in the top level CR, the mutate
webhook invocation creates an instance of a `glanceAPI` object called "default",
and it results in two GlanceAPI deployments.

```
apiVersion: glance.openstack.org/v1beta1
kind: Glance
...
spec:
  serviceUser: glance
  databaseInstance: openstack
  databaseUser: glance
  glanceAPIs:
    ..
  secret: osp-secret
  storageClass: ""
...
...
```

Both `GlanceAPI` `internal` and `external` sub-CR(s) are generated, resulting in
two different k8s `StatefuSet` objects.
The `Status` and the definition of the deployed `GlanceAPI` services can be
retrieved with the following commands:

```
$ oc get Glance

NAME     STATUS   MESSAGE
glance   True     Setup complete

$ oc get GlanceAPI

NAME                      NETWORKATTACHMENTS   STATUS   MESSAGE
glance-default-external                        True     Setup complete
glance-default-internal                        True     Setup complete
```

The resulting CRs can be inspected as regular k8s objects.
The following diagram describes how k8s resources/object are connected.

```
                                                         [statefulset(s)]
                                                   +-------------------------+
+------------+       +---------------------+ |---> | glance-default-internal |
| glance CR  | |---> | glanceAPI-default CR|       +-------------------------+
+------------+       +---------------------+ |---> | glance-default-external |
                                                   +-------------------------------+
 (headless SVC to allow pod2pod communication)     | glance-default-internal-api   |
 (it must match with the StatefulSet ServiceName)  +-------------------------------+
                                                   | glance-default-external-api   |
                                                   +-------------------------------+

```

By default, if no option is specified, a `glanceAPI` deployment is `split`
between `internal` and `external`.
It's possible to not `split` the deployment and reduce everything to a `single`
`API` instance.
To do that, a `Glance` CR spec like the following should be created:

```
apiVersion: glance.openstack.org/v1beta1
kind: Glance
...
spec:
  serviceUser: glance
  databaseInstance: openstack
  databaseUser: glance
  glanceAPIs:
    api0:
      replicas=1
      type: "single"
  secret: osp-secret
  storageClass: ""
...
...
```

As per the example above, by passing `type: single` to the `api0` instance,
defined in the `glanceAPIs` section, let the main `Glance` controller to only
generate a single `api0` `StatefulSet` that is not split between `internal`
and `external`.

```
                                                                               [k8s Service(s)]
                                                   [statefulset(s)]        +---------------------+
+------------+       +------------------+       +-------------------+      | glance-api-internal |
| glance CR  | |---> | glance-default CR| |---> | glance-api-single | |--> | --------------------+
+------------+       +------------------+       +-------------------+      | glance-api-public   |
                                                                           +-----------------------+
                                                       (headless Svc) |--> | glance-api-single-api |
                                                                           +-----------------------+

```

Both `internal` and `public` k8s `Services` are created, and they point to the
single `StatefuSet` generated by the `glanceAPI` controller.

```
$ oc get glanceapi
NAME                NETWORKATTACHMENTS   STATUS   MESSAGE
glance-api-single                        True     Setup complete
```

The generic `GlanceAPI` StatefuSet, generated by the process above, deploys three
containers within the same Pod:

```
+-------------------------------------+
| GlanceAPI::api0:Pod                 |
|                                     |
|   +-----------------------+         |
|   |   +------------+      |         |
|   |   | glance-log | -------------------> It streams the glance-api0 logs
|   |   +------------+      |         |     in /var/log/glance
|   +-----------------------+         |
|   +-----------------------+         |
|   |   +--------------+    |         |
|   |   | glance-httpd | ---|-------------> It intercepts requests on 9292 and
|   |   +-+------------+    |         |     do the ProxyPass to the process
|   |     |                 |         |     behind on 127.0.0.1:9293
|   |   +-+----------+      |         |
|   |   | glance-api | -----|-------------> Runs the glance-api process on 9293
|   |   +------------+      |         |
|   +-----+-----------------+         |
+---------|-----------------------------+
          |
          +-> GlanceAPI:api0 = (httpd + glance-api)
```

The reason about having an additional `httpd` layer relies on the fact that
`TLS` is not natively supported in Glance.
Even though Glance it's not supported in production as regular wsgi process
run by httpd, this scenario sees httpd configured in a way that executes
`ProxyPass` for the requests coming from the associated endpoint to the
glance-api service.

More details about this can be found in the
[upstream documentation](https://docs.openstack.org/glance/latest/admin/apache-httpd.html).


## Deploy multiple GlanceAPI instances

The previous section describes the deployment flow and how many layouts are
supported by a `glanceAPI`: having `single` or `split` might change according
to the configured backend.
An additional feature provided by the `glance-operator` is the ability to deploy
multiple `GlanceAPI` that are supposed to serve different workloads.
A real use case scenario where it makes sense to have multiple `GlanceAPI` is an
`Edge` deployment (known as `DCN`).
This is a scenario where multiple `glanceAPI` instances are orchestrated by the
same operator, but they're connected with different backends.

```
spec:
  customServiceConfig: |
    [DEFAULT]
    enabled_backends = default_backend:rbd
    [glance_store]
    default_backend = default_backend
    [default_backend]
    rbd_store_ceph_conf = /etc/ceph/ceph.conf
    store_description = "RBD backend"
    rbd_store_pool = images
    rbd_store_user = openstackcustomServiceConfig: |
  databaseInstance: openstack
  databaseUser: glance
  keystoneEndpoint: central
  glanceAPIs:
    central:
      replicas: 1
    edge0:
      replicas: 1
    edge1:
      replicas: 1
...
...
...
  extraMounts:
    - name: central
      region: r1
      extraVol:
        - propagation:
          - central
          volumes:
          - name: ceph0
            projected:
              sources:
              - secret:
                  name: ceph-conf-files-0
          mounts:
          - name: ceph0
            mountPath: "/etc/ceph"
            readOnly: true
    - name: edge0
      region: r1
      extraVol:
        - propagation:
          - edge0
          volumes:
          - name: ceph1
            projected:
              sources:
              - secret:
                  name: ceph-conf-files-1
          mounts:
          - name: ceph1
            mountPath: "/etc/ceph"
            readOnly: true
    - name: edge1
      region: r1
      extraVol:
        - propagation:
          - edge1
          volumes:
          - name: ceph2
            projected:
              sources:
              - secret:
                  name: ceph-conf-files-2
          mounts:
          - name: ceph2
            mountPath: "/etc/ceph"
            readOnly: true
```

In the example above, all the `GlanceAPI` instances share the same configuration,
which is inherited by the main `customServiceConfig`, while`extraMounts` are
added to connect each instance to a different Ceph cluster.
For each instance it's possible to configure the `layout` (split vs single)
according to a given backend, but note that `webhooks` prevent any update to the
defined layout.


## Manage KeystoneEndpoint

Even though multiple instances can be deployed and be reachable through the generated
k8s `Services`, only one of them can be registered in the keystone catalog at a given
time.
There's a `1:1` relation between the `image` service in keystone, associated to Glance,
and the `Endpoint` (`internal` and `public`) that exist in `Keystone`.
For this reason, in the top-level `Glance` CR, a `keystoneEndpoint` parameter is
defined and exposed.
Unless a `single` instance is deployed, the human operator is supposed to choose,
before the main `OpenStackControlPlane` `CR` is applied, which instance should be
registered in keystone.

As an example, let's consider a generic `Glance` spec like the following:

```
spec:
  customServiceConfig: |
  ...
  ...
  databaseInstance: openstack
  databaseUser: glance
  keystoneEndpoint: api0
  glanceAPIs:
    api0:
      replicas: 1
    api1:
      replicas: 1
    api2:
      replicas: 1
...
...
```

The `keystoneEndpoint` parameter points to `api0`: it means that `api0` will be
registered in the Keystone catalog.
An OpenStack administrator will be able to reach that instance via `openstack-cli`
and perform any normal `image` related operation.
At the same time, `api1` and `api2` are not reachable via cli, and they will only
be available through their k8s `Services` by applications that are able to
discover their `Endpoints`.
Once a `GlanceAPI` is deployed, its `Endpoints` are reflected in both the generated
sub-CR, as well as in the top level CR.
For each deployed API instance, an entry is added in the `Status.Endpoints`
map, and each endpoint is prefixed with the name of the API it refers to.

```
apiVersion: glance.openstack.org/v1beta1
kind: Glance
...
status:
  apiEndpoint:
    api0-internal: http://glance-api0-internal.openstack.svc:9292
    api0-public: https://glance-api0-public.apps.crc.openstack.dev:9292
    api1-internal: http://glance-api1-internal.openstack.svc:9292
    api1-public: https://glance-api1-public.apps.crc.openstack.dev:9292
    ...
...
```

When an `API` is decommissioned, the associated `Endpoint` is removed from the
top-level `apiEndpoint` map.
Other than decommissioning an instance (delete from the main CR), it's possible,
at any point in time, to update the `keystoneEndpoint` value and select a different
instance that should be registered in Keystone.
An update to the `keystoneEndpoint` parameter results in a reconciliation execution
that switches the two affected instances (the one registered in keystone and the
one proposed by the new update).

## How Conditions are managed

Conditions represent a critical aspect for reconciliation because they allow:

1. to evaluate the status of a particular component validating a set of conditions
2. give a feedback to the end user about the current state of the Deployment
3. identify the status of the underlying GlanceAPIs and report their healhy to
   the upper level CR

The document
[docs/conditions](https://github.com/openstack-k8s-operators/docs/blob/main/conditions.md)
describes the meaning of the k8s-operators shared `Conditions`.
Conditions are re-evaluated when a new `Reconcile` loop starts, and if one of
them is different from what was previously registered, an update is performed,
and it allows to give immediate feedback to the user.
Conditions are initially marked as `Unknown`, while the general `ReadyCondition`
is marked as `False` because we assume that the `Deployment` is in progress but
not `Ready` at the same time, until the entire set of conditions are evaluated.
`Conditions` can be seen as a checklist or steps within the reconcile loop, and
while the loop proceed to the end, each of them is evaluated.
If a particular component with an associated condition is not `Ready`, an `error`
is returned from the operator, and its failing condition is marked as `False`
with an appropriate error message.
If the end of the loop is reached, it means we passed through all the steps and
the `Conditions` are marked to `True`: it is possible, at this point, to mark the
overall `ReadyCondition` to `Status=True` as well and `Mirror` the result to the
top-level CR.
`Conditions` are always Mirrored using the defer function, so that we always
have an information to the `ReadyCondition` that reflects the point where the
loop is exiting/failing.
The general `IsReady` function is used as a wrapper for the `ReadyCondition`
boolean, and it can be used to get the status of the `instance`. In general
in the glance-operator we make the following assumptions:
- A Glance/GlanceAPI is considered `Ready` if all the subconditions are verified
- A Deployment (intended as the `StatefulSet` that represents the GlanceAPIs) is
  considered `Ready` if the number of `Replicas` specified in the `Glance` CR
  spec is **equal** to the number of available instances (`ReadyCount`).
